import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models

(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()
X_train = (X_train.astype(np.float32) - 127.5) / 127.5
X_train = np.expand_dims(X_train, axis=-1)

latent_dim = 100
num_examples_to_generate = 16

def build_generator():
    model = models.Sequential()
    model.add(layers.Dense(256, input_dim=latent_dim))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(512))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(1024))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(28 * 28 * 1, activation='tanh'))
    model.add(layers.Reshape((28, 28, 1)))
    return model

def build_discriminator():
    model = models.Sequential()
    model.add(layers.Flatten(input_shape=(28, 28, 1)))
    model.add(layers.Dense(512))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dense(256))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

generator = build_generator()
discriminator = build_discriminator()
discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

discriminator.trainable = False
gan_input = layers.Input(shape=(latent_dim,))
generated_image = generator(gan_input)
gan_output = discriminator(generated_image)
gan = models.Model(gan_input, gan_output)
gan.compile(optimizer='adam', loss='binary_crossentropy')

def generate_and_save_images(model, epoch, test_input):
    predictions = model(test_input)
    predictions = (predictions.numpy() + 1) / 2
    plt.figure(figsize=(4, 4))
    for i in range(predictions.shape[0]):
        plt.subplot(4, 4, i + 1)
        plt.imshow(predictions[i, :, :, 0], cmap='gray')
        plt.axis('off')
    plt.savefig(f'gan_epoch_{epoch}.png')
    plt.show()

def train_gan(epochs, batch_size):
    random_latent_vectors = tf.random.normal(shape=(num_examples_to_generate, latent_dim))
    for epoch in range(epochs):
        idx = np.random.randint(0, X_train.shape[0], batch_size)
        real_images = X_train[idx]
        noise = tf.random.normal(shape=(batch_size, latent_dim))
        fake_images = generator(noise)
        combined_images = tf.concat([real_images, fake_images], axis=0)
        labels = tf.constant([[1.0]] * batch_size + [[0.0]] * batch_size)
        d_loss = discriminator.train_on_batch(combined_images, labels)
        noise = tf.random.normal(shape=(batch_size, latent_dim))
        misleading_labels = tf.constant([[1.0]] * batch_size)
        g_loss = gan.train_on_batch(noise, misleading_labels)
        if epoch % 100 == 0:
            print(f"Epoch: {epoch}")
            print(f"Discriminator Loss: {d_loss[0]}")
            print(f"Generator Loss: {g_loss}")
            generate_and_save_images(generator, epoch, random_latent_vectors)

epochs = 300
batch_size = 64
train_gan(epochs, batch_size)
